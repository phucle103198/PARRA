{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phucle103198/PARRA/blob/main/AssessConsistencyData_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxR_g_cPKMVl"
      },
      "source": [
        "# Import libarary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufhr8bfOKMVn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NONa2OZ2KMVo"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using directly dataset on my drive"
      ],
      "metadata": {
        "id": "2kdLtIq5Lprk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"19a_wEMoK_6VXrJnrbAgOnXf_yQMns7mE\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "df = pd.read_excel(url, sheet_name=\"Sheet1\")\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "qOzKli1KLaOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IQKFCJuKMVp"
      },
      "source": [
        "# Load model Zero-shot NLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWPHZZkpKMVq"
      },
      "source": [
        "##### Some model using class 0 is entailment and 2 is contradiction, you should focus this infomation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"Phucle103198/PARRA\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "print(model.config.id2label)\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "04ERQnPkS5aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Or if you want to try another model, you can download via Huggingface :"
      ],
      "metadata": {
        "id": "spgppmhxTseI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtrLQrYVKMVq"
      },
      "outputs": [],
      "source": [
        "''' Rename model_name from Huggingface and run this code\n",
        "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qrTSsqXKMVr"
      },
      "source": [
        "# Start with STEP 1: Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsp-PPEtKMVr"
      },
      "outputs": [],
      "source": [
        "hedonic_labels =  [\"I don't like it\", 'I like it moderately', 'I like it a lot']\n",
        "hypotheses1 = [f\"The consumer of this product said {label}.\" for label in hedonic_labels]\n",
        "results1 = []\n",
        "for i in df['FreeJAR_description'].tolist():\n",
        "    premise = i\n",
        "    ## premise-hypothesis\n",
        "    pairs1 = [(premise, hypothesis) for hypothesis in hypotheses1]\n",
        "    ## Token\n",
        "    inputs1 = tokenizer(pairs1, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    # Run model to get logits\n",
        "    with torch.no_grad():\n",
        "        outputs1 = model(**inputs1)\n",
        "        logits1 = outputs1.logits\n",
        "    entailment_probs1 = torch.softmax(logits1, dim=0)[:,0]\n",
        "    results1.append(entailment_probs1)\n",
        "df_probs = pd.DataFrame([t.tolist() for t in results1], columns=hedonic_labels)\n",
        "res = pd.concat([df, df_probs], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvMRKc4KMVr"
      },
      "source": [
        "# Next to STEP 2: Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VucpGhplKMVr"
      },
      "source": [
        "### Post-processing Predicted Results\n",
        "\n",
        "To better analyze the model's outputs, we will perform some post-processing steps:\n",
        "\n",
        "#### 1. Extract the Predicted Label\n",
        "For each row, we will select the label with the highest predicted probability and assign it to a new column called **\"Predict\"**.\n",
        "\n",
        "#### 2. Evaluate Consistency\n",
        "We will compare the predicted label with the label chosen by the consumer:\n",
        "- If they match, we consider it **\"consistent\"**.\n",
        "- If they differ, we label it as **\"inconsistent\"**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XQk_43EKMVr"
      },
      "outputs": [],
      "source": [
        "res['predict'] = res.iloc[:,-3:].idxmax(axis=1)\n",
        "res['consistency'] = res.apply(lambda row: 'consistent' if row['Hedonic_category'] == row['predict'] else 'inconsistent', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3y42SZTKMVr"
      },
      "source": [
        "### Model Mistake Evaluation\n",
        "Basically, when the model's prediction does not match the selected label, we consider it a mistake from the model's perspective. In this context, we define two levels of mistakes:\n",
        "#### 1. Serious Mistake\n",
        "This occurs when the model predicts something completely opposite to the actual label — for example, predicting **\"I don't like it\"** when the true label is **\"I like it a lot\"**, or vice versa. This indicates a major misjudgment by the model.\n",
        "#### 2. Non-Serious Mistake\n",
        "This happens when the model's prediction is only slightly off — for example, predicting **\"I like it moderately\"** instead of **\"I like it a lot\"**.\n",
        "\n",
        "---\n",
        "\n",
        "### Formal Definition\n",
        "Consider the labels as ordered categories:\n",
        "- `0` → \"I don't like it\"  \n",
        "- `1` → \"I like it moderately\"  \n",
        "- `2` → \"I like it a lot\"\n",
        "\n",
        "We define mistake severity based on the **distance between the predicted label index and the true label index**:\n",
        "- If the difference is `2`: **Serious mistake**\n",
        "- If the difference is `1`: **Nonserious mistake**\n",
        "- If the difference is `0`: ✅ **Correct prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f3mF6K3KMVs"
      },
      "outputs": [],
      "source": [
        "res[\"mistake\"] = res['Hedonic_category'] != res['predict']\n",
        "res[\"serious_mistake\"] = ((res['Hedonic_category'] == \"I don't like it\") & (res['predict'] == \"I like it a lot\")) | ((res['Hedonic_category'] == \"I like it a lot\") & (res['predict'] == \"I don't like it\"))\n",
        "res[\"nonserious_mistake\"] = ((res['Hedonic_category'] == \"I like it moderately\") & ((res['predict'] == \"I like it a lot\") | (res['predict'] == \"I don't like it\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4_546wGKMVs"
      },
      "source": [
        "### Now we have dataframe look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IED0mmU9KMVs"
      },
      "outputs": [],
      "source": [
        "res.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1gI5IjMKMVs"
      },
      "source": [
        "# Assess consistency of the data\n",
        "## Global consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SfwTr9dKMVs"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5,5))\n",
        "consistent_value = res['consistency'].value_counts()['consistent']*100/len(res)\n",
        "sizes = [consistent_value, 100-consistent_value]\n",
        "colors = ['#1E90FF', '#E0E0E0']  # màu chính và màu nền\n",
        "labels = ['Consistent', 'Inconsistent']\n",
        "ax.pie(sizes, labels=labels, colors=colors, startangle=90, counterclock=False, wedgeprops={'width': 0.4})\n",
        "ax.text(0, 0, '{}%'.format(round(consistent_value,2)), ha='center', va='center', fontsize=18, weight='bold')\n",
        "fig.suptitle(\"Global consistency\")\n",
        "plt.axis('equal')  # make sure that circle\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLT7nMvdKMVt"
      },
      "source": [
        "##### Global consistency: a crucial metric that reflects how coherent and interpretable a comment is from the model’s perspective. It serves as an overall consistency index, capturing the unified meaning of the entire comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7AJf4-1KMVt"
      },
      "source": [
        "## Class-level consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CF9pPl9KMVt"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha54tkyLKMVt"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = np.array(hedonic_labels) ### Make sure that x-axis and y-axis have same order like Hedonic labels\n",
        "categories_encoded = label_encoder.transform(res['Hedonic_category'])\n",
        "original_classes = list(label_encoder.classes_)\n",
        "\n",
        "swapped_classes = [original_classes[0], original_classes[2], original_classes[1]]\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(res['Hedonic_category'], res['predict'], labels=swapped_classes)\n",
        "# Create a more visually appealing plot\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='OrRd',\n",
        "            xticklabels=swapped_classes,\n",
        "            yticklabels=swapped_classes, cbar=False,\n",
        "            annot_kws={\"size\": 16})\n",
        "plt.title('Confusion Matrix', pad=20, fontsize=18)\n",
        "plt.xlabel('Predicted Label', labelpad=10, fontsize=10)\n",
        "plt.ylabel('True Label', labelpad=10, fontsize=10)\n",
        "plt.xticks(rotation=30)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU7rNf5DKMVt"
      },
      "source": [
        "#### Plot consistency per each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArPs_m8ZKMVt"
      },
      "outputs": [],
      "source": [
        "colors = ['#FF6600', '#E0E0E0']  # change color in here\n",
        "labels = ['Consistent', 'Inconsistent']\n",
        "fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(15,5))\n",
        "## I don't like it\n",
        "res_temp = res[res['Hedonic_category'] == \"I don't like it\"]\n",
        "consistent_value = res_temp['consistency'].value_counts()['consistent']*100/len(res_temp)\n",
        "sizes = [consistent_value, 100-consistent_value]\n",
        "axs[0].pie(sizes, labels=labels, colors=colors, startangle=90, counterclock=False, wedgeprops={'width': 0.4})\n",
        "axs[0].text(0, 0, '{}%'.format(round(consistent_value,2)), ha='center', va='center', fontsize=18, weight='bold')\n",
        "axs[0].set_title(\"Consistency of class I don't like it\")\n",
        "axs[0].axis('equal')  # make sure the circle is not distorted\n",
        "## I like it moderately\n",
        "res_temp = res[res['Hedonic_category'] == \"I like it moderately\"]\n",
        "consistent_value = res_temp['consistency'].value_counts()['consistent']*100/len(res_temp)\n",
        "sizes = [consistent_value, 100-consistent_value]\n",
        "axs[1].pie(sizes, labels=labels, colors=colors, startangle=90, counterclock=False, wedgeprops={'width': 0.4})\n",
        "axs[1].text(0, 0, '{}%'.format(round(consistent_value,2)), ha='center', va='center', fontsize=18, weight='bold')\n",
        "axs[1].set_title(\"Consistency of class I like it moderately\")\n",
        "axs[1].axis('equal')  # make sure the circle is not distorted\n",
        "## I like it a lot\n",
        "res_temp = res[res['Hedonic_category'] == \"I like it a lot\"]\n",
        "consistent_value = res_temp['consistency'].value_counts()['consistent']*100/len(res_temp)\n",
        "sizes = [consistent_value, 100-consistent_value]\n",
        "axs[2].pie(sizes, labels=labels, colors=colors, startangle=90, counterclock=False, wedgeprops={'width': 0.4})\n",
        "axs[2].text(0, 0, '{}%'.format(round(consistent_value,2)), ha='center', va='center', fontsize=18, weight='bold')\n",
        "axs[2].set_title(\"Consistency of class I like it a lot\")\n",
        "axs[2].axis('equal')  # make sure the circle is not distorted\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKDryhMYKMVu"
      },
      "source": [
        "#### This analysis examines the consistency of predictions within each hedonic category, providing insight into how distinctly consumers express their preferences in text.\n",
        "#### Notably, the category \"I like it moderately\" shows lower consistency, likely due to the inherently ambiguous nature of moderate liking. Such comments often contain mixed sentiments, which makes it more challenging for semantic models to align with a clear emotional signal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOTWMalOKMVu"
      },
      "source": [
        "## Judge-level consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDrtA7yvKMVu"
      },
      "outputs": [],
      "source": [
        "res.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV2rZ16kKMVu"
      },
      "outputs": [],
      "source": [
        "mistake_summary = res.groupby('ID_judge')[['mistake', 'serious_mistake']].sum().reset_index()\n",
        "serious_mistake_count = mistake_summary.groupby('mistake')['serious_mistake'].sum().reindex(range(0, mistake_summary['mistake'].max() + 1), fill_value=0)\n",
        "mistake_count = mistake_summary['mistake'].value_counts().reindex(range(0, mistake_summary['mistake'].max() + 1), fill_value=0)\n",
        "nonserious_mistake_count = mistake_count - serious_mistake_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b5u4jrpKMVu"
      },
      "source": [
        "#### In this case, we set the threshold is 4, which is half of the number of product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xygHY7ZKMVu"
      },
      "outputs": [],
      "source": [
        "wrong_predict = np.array(mistake_count.index)\n",
        "weight_counts = {\n",
        "    \"Non-serious\": np.array(nonserious_mistake_count),\n",
        "    \"Serious\": np.array(serious_mistake_count),\n",
        "}\n",
        "colors = {\n",
        "    \"Non-serious\": \"#6EC6FF\",\n",
        "    \"Serious\": \"#FF6F61\",\n",
        "}\n",
        "width = 0.6\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "# stacking\n",
        "bottom = np.zeros(len(wrong_predict))\n",
        "for label, weight_count in weight_counts.items():\n",
        "    ax.bar(wrong_predict, weight_count, width, label=label, bottom=bottom, color=colors[label])\n",
        "    bottom += weight_count\n",
        "ax.vlines(x=4, ymin=0, ymax=30, linestyles=\"dashdot\", colors=\"red\")\n",
        "ax.set_xlabel(\"Number of wrong answers per judge \", fontsize=14)\n",
        "ax.set_ylabel(\"Number of judge\", fontsize=14)\n",
        "ax.legend(title=\"Type of mistake\", fontsize=12)\n",
        "ax.set_xticks(wrong_predict)\n",
        "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "ax.set_ylim(0,40)\n",
        "for i, v in enumerate(mistake_count):\n",
        "    ax.text(i, v + 0.5, str(v), ha='center', fontsize=16, fontweight='bold')\n",
        "for spine in ['top', 'right']:\n",
        "    ax.spines[spine].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.title(\"Classification of errors by the number of mistakes per judge\", fontsize=16, fontweight='bold')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPS4aF5sKMVv"
      },
      "source": [
        "### Detail of 2 serious mistake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnSEMMSRKMVv"
      },
      "outputs": [],
      "source": [
        "res[res['serious_mistake'] == True][['Hedonic_category', 'FreeJAR_description']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SnrQpJMKMVv"
      },
      "source": [
        "##### In both cases, the judge selected \"I like it a lot\", yet their written descriptions suggest otherwise. Interestingly, while the model strongly predicts \"I don't like the product\", this also seems inconsistent with the sentences. A more appropriate classification might be \"I like it moderately\", which better reflects the nuanced tone of the feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdFSdR_0KMVv"
      },
      "source": [
        "### Who Makes More Mistakes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f18iSIL8KMVw"
      },
      "outputs": [],
      "source": [
        "mistake_summary[mistake_summary['mistake']>=4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUTGTspiKMVw"
      },
      "outputs": [],
      "source": [
        "res[(res['ID_judge'].isin([33003, 33011, 33065])) & (res['consistency'] == 'inconsistent')][['Hedonic_category', 'FreeJAR_description', 'predict']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNsi0HxtKMVx"
      },
      "source": [
        "## Judge-Product level Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmFfdMobKMVx"
      },
      "source": [
        "### Consistency Analysis: Panelists and Products\n",
        "\n",
        "Inspired by the *panel performance* functionality in the **SensomineR** package, we adopt a similar approach to assess how consistently individual testers (panelists) evaluate products, note that from the perspective of a predictive model.\n",
        "\n",
        "In this analysis, we take a slightly different approach compared to the previous section.\n",
        "\n",
        "Instead of using the **highest predicted label** (i.e., the label with the highest probability), we focus on the **model's confidence in the label actually selected by the tester**.\n",
        "\n",
        "That is, for each prediction, we extract the probability assigned by the model to the label that the human tester chose — regardless of whether it was the top prediction or not.\n",
        "\n",
        "#### Objectives of This Analysis\n",
        "\n",
        "- **Identify Consistent vs. Inconsistent Testers**  \n",
        "  We aim to evaluate the level of agreement between the model's predictions and each tester's selected label. This allows us to identify:\n",
        "  - **Consistent individuals**, whose responses align well with the model.\n",
        "  - **Inconsistent individuals**, whose evaluations frequently differ from the model’s predictions.\n",
        "\n",
        "- **Assess Product-Level Consistency**  \n",
        "  We also apply this analysis at the product level. Specifically, we examine:\n",
        "  - Which products tend to yield **high agreement** between model predictions and tester labels (i.e., easier to evaluate consistently).\n",
        "  - Which products result in **high disagreement**, indicating that they may be more subjective or polarizing among testers.\n",
        "\n",
        "This two-layered analysis helps us understand not only the reliability of individual testers, but also which products provoke more diverse or inconsistent evaluations—insightful for both product development and panel management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RGst5wSKMVx"
      },
      "outputs": [],
      "source": [
        "def get_prob_chosen(row):\n",
        "    return row[f\"{row['Hedonic_category']}\"]\n",
        "res[\"prob_chosen\"] = res.apply(get_prob_chosen, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLMhd9qGKMVx"
      },
      "outputs": [],
      "source": [
        "pivot = res.groupby([\"ID_judge\", \"Product\"])[\"prob_chosen\"].mean().unstack()\n",
        "pivot[\"Median\"] = pivot.median(axis=1)\n",
        "median_row = pivot.median(axis=0)\n",
        "median_row.name = \"Median\"\n",
        "pivot_with_median = pd.concat([pivot, median_row.to_frame().T])\n",
        "sorted_cols = pivot_with_median.loc[\"Median\"].sort_values(ascending=False).index\n",
        "sorted_rows = pivot_with_median[\"Median\"].sort_values(ascending=False).index\n",
        "sorted_df = pivot_with_median.loc[sorted_rows, sorted_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqmi-QfIKMVx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(15,20))\n",
        "ax = sns.heatmap(sorted_df.drop(columns=\"Median\", index=\"Median\"), annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Consistency score'}, )\n",
        "ax.collections[0].cmap.set_bad('0.7')\n",
        "plt.xlabel(\"Product\")\n",
        "plt.ylabel(\"judge\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtZomRsJKMVx"
      },
      "source": [
        "#### - From the product perspective, it is evident that products M, E and S demonstrate higher consistency in the results compared to B, CR and R. This suggests that M and E may be easier for testers to evaluate reliably, possibly due to clearer sensory characteristics or more distinct differences. In contrast, CR and R might have subtle or ambiguous attributes that lead to more variability in responses.\n",
        "\n",
        "#### - From the Judge perspective, individuals 33067 and 33087 exhibit a higher degree of consistency in their evaluations, whereas testers 33100, 33006, and 33005 show more variation in their responses. This inconsistency could be attributed to factors such as lack of concentration, fatigue, or unclear understanding of the test protocol, all of which may affect their ability to evaluate the products consistently.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}